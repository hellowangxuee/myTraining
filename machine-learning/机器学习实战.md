算法
------------
### K-近邻算法
K-近邻算法采用测量不同特征值之间的距离方法进行分类。  
K-近邻算法(KNN)工作原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即样本集中每一数据与所属分类的对应关系。
输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据(最近邻)的分类标签。通常选择
样本数据集中前 K 个最相似的数据，且 K 是不大于 20  的整数，最后选择 K 个中出现次数最多的分类，作为新数据的分类。  
#### 准备：使用Python 导入数据
在KNN.py 中增加如下代码：代码中导入了两个包：科学计算包 NumPy;运算符模块  
```Python
from numpy import *
import operator

def createDataSet():
	group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
	labels = ['A','A','B','B']
	return group, labels
```
切换到 KNN.py 所在的文件夹，  
```Python
>>> import KNN
>>> group,labels = KNN.createDataSet()
>>> group
array([[ 1. ,  1.1],
       [ 1. ,  1. ],
       [ 0. ,  0. ],
       [ 0. ,  0.1]])
>>> labels
['A', 'A', 'B', 'B']
```
为了简单，通常每个数据点只使用两个特征。  
向量 labels 包含了每个数据点的标签信息， labels 包含的元素个数等于 group 矩阵行数。  
#### 实施 KNN 算法
##### 对未知类别属性的数据集中的每个点依次执行以下操作：
1. 计算一直类别数据据中的店与当前点之间的距离；
2. 按照距离递增依次排序；
3. 选取与当前距离最小的 K 个点；
4. 确定前 K 个点所在类别的出现频率；
5. 返回前 K 个点出现频率最高的类别作为当前的预测分类。
```python
def classify0(inX, dataSet, labels, k):
	dataSetSize = dataSet.shape[0]
	diffMat = tile(inX, (dataSetSize,1)) - dataSet
	sqDiffMat = diffMat**2
	sqDistances = sqDiffMat.sum(axis=1)
	distances = sqDistances**0.5
	sortedDistIndicies = distances.argsort()
	classCount = {}
	for i in range (k):
		voteIlabel = labels[sortedDistIndicies[i]]
		classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
	sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
	return sortedClassCount[0][0]
```
classify0() 函数有四个输入参数：用于分类的输入向量是 inX；输入的训练样本集为 dataSet；标签向量为 labels；最后参数 K 表示用于选择最近邻居的数目。  
```Python
>>> import KNN
>>> group, labels = KNN.createDataSet()
>>> KNN.classify0([0,0], group, labels, 3)
'B'
```
到目前为止，已经构造了第一个分类器。  
k-近邻算法是分类数据最简单最有效的算法，他是基于实例的学习，使用算法时必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练
数据集很大，必须使用大量的存储空间。另外，他无法给出任何数据的基础结构信息，因此无法知晓实例样本和典型实例样本具有什么特征。  
### 决策树
#### 适用数据类型
数值型和标称型。  
#### 优点
计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。  
#### 缺点
可能会产生过度匹配问题。  
构造决策树时需要找到划分数据的决定性特征，此处采用 ID3算法。  
#### 信息增益
划分数据集原则：将无序的数据变得更加有序。   
划分数据集前后信息发生的变化称为信息增益。获得信息增益最高的特征就是最好的选择。  
#### 熵
定义：信息的期望值  
#### 计算香农熵
```python
from math import log
def calcShannonEnt(dataSet):
	numEntries = len(dataSet)
	labelCounts = {}
	for featVec in dataSet:
		currentLabel = featVec[-1]
		if currentLabel not in labelCounts.keys():
			labelCounts[currentLabel] = 0
		labelCounts[currentLabel] +=1
	shannonEnt = 0.0
	for key in labelCounts:
		prob = float(labelCounts[key])/numEntries
		shannonEnt -= prob * log(prob,2)
	return shannonEnt
	
def createDataSet():
	dataSet = [[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']]
	labels = ['no surfacing','flippers']
	return dataSet, labels
```
在命令行下输入如下命令：  

```python
>>> import trees
>>> myDat,labels = trees.createDataSet()
>>> myDat
[[1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
>>> trees.calcShannonEnt(myDat)
0.8112781244591328
```
熵越高，则混合的数据越多。 
#### 基尼不纯度
从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。  
#### 划分数据集
```python
def chooseBestFeatureToSplit(dataSet):
	numFeatures = len(dataSet) - 1
	baseEntropy = calcShannonEnt(dataSet)
	bestInfoGain = 0.0
	bestFeature = -1
	for i in range(numFeatures):
		featList = [example[i] for example in dataSet]
		uniqueVals = set(featList)
		newEntropy = 0.0
		for value in uniqueVals:
			subDataSet = splitDataSet(dataSet, i, value )
			prob = len(subDataSet) / float(len(dataSet))
			newEntropy += prob * calcShannonEnt(subDataSet)
		infoGain = baseEntropy - newEntropy
		if(infoGain > bestInfoGain):
			bestInfoGain = infoGain
			bestFeature = i
	return bestFeature
```
运行下面的代码:  

```python
>>> import trees
>>> myDat, labels = trees.createDataSet()
>>> trees.chooseBestFeatureToSplit(myDat)
0
```
运行结果显示，第0个特征是最好的用于划分数据集的特征。  


### 基于概率论的分类方法：朴素贝叶斯

#### 使用python进行文本分类

#### 准备数据

在bayes.py中保存下面代码

```python

    #encoding:utf-8
    
    def loadDataSet():
        #"函数返回值第一个变量是进行词条切分后的文档集合，第二个变量是类别标签集合"
        postingList =[
            ['my','dog','has','flea','problem','help','please'],
            ['maybe','not','take','him','to','dog','park','stupid'],
            ['my','dalmation','is','so','cute','I','love','him'],
            ['stop','posting','stupid','worthless','garbage'],
            ['mr','licks','ate','my','steak','how','to','stop','him'],
            ['quit','buying','worthless','dog','food','stupid']
        ]
        classVec = [0,1,0,1,0,1]   #1 代表侮辱性文字，0  代表正常言论
        return postingList, classVec
    
    def createVocabList(dataSet):
        #"创建一个包含所有文档中出现的不重复词的列表"
        vocabSet = set([])
        for document in dataSet:
            vocabSet = vocabSet | set(document)   #创建集合的并集
        return list(vocabSet)
    
    def setOfWords2Vec(vocabList, inputSet):
        returnVec = [0] * len(vocabList)          #创建一个所含元素都为0的向量
        for word in inputSet:
            if word in vocabList:
                returnVec[vocabList.index(word)] = 1
            else:
                print "the word:%s is not in my Vocabulary!" %word
        return returnVec

```

运行效果

```python

import bayes

listOposts,listClasses = baes.loadDataSet()

myVocabList = baes.createVocabList(listOposts)

myVocabList

['cute', 'love', 'help', 'garbage', 'quit', 'I', 'stop', 'is', 'park', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'problem', 'steak', 'my']

baes.setOfWords2Vec(myVocabList, listOposts[0])

[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1]

baes.setOfWords2Vec(myVocabList, listOposts[3])

[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]

```

使用词汇表所有单词作为输入，然后为每个单词建立一个特征，一旦给定一篇文档，就会被转换为词向量。myVocabList中索引为2的单词是 help，该单词在第一篇文档中出现但是在第四篇文档中未出现。

训练算法：从词向量计算概率

bayes.py中导入numpy，并添加下列代码
```python

    def trainNBO(trainMatrix, trainCategory):
        "trainMatrix:文档矩阵；trainCategory:每篇文档类别标签所构成的向量"
        numTrainDocs = len(trainMatrix)
        numWords = len(trainMatrix[0])
        pAbusive = sum(trainCategory)/float(numTrainDocs)
        p0Num = zeros(numWords)
        p1Num = zeros(numWords)                      #初始化概率
        p0Denom = 0.0
        p1Denom = 0.0
        for i in range(numTrainDocs):
            if trainCategory[i] == 1:                #向量相加
                p1Num += trainMatrix[i]
                p1Denom += sum(trainMatrix[i])
            else:
                p0Num += trainMatrix[i]
                p0Denom += sum(trainMatrix[i])
        p1Vect = p1Num/p1Denom                        #对每个元素做除法
        p0Vect = p0Num/p0Denom
        return p0Vect,p1Vect,pAbusive

```

```python

import bayes

listOPosts,listClasses = bayes.loadDataSet()

myVocabList = bayes.createVocabList(listOPosts)

trainMat = []

for postinDoc in listOPosts:#对每篇文档都计算词向量

...     trainMat.append(bayes.setOfWords2Vec(myVocabList, postinDoc))

p0V,p1V,pAb = bayes.trainNBO(trainMat,  listClasses)

pAb

0.5

p0V

array([ 0.04166667,  0.04166667,  0.04166667,  0.        ,  0.        ,

        0.04166667,  0.04166667,  0.04166667,  0.        ,  0.04166667,
        0.04166667,  0.04166667,  0.        ,  0.        ,  0.08333333,
        0.        ,  0.        ,  0.04166667,  0.        ,  0.04166667,
        0.04166667,  0.        ,  0.04166667,  0.04166667,  0.04166667,
        0.        ,  0.04166667,  0.        ,  0.04166667,  0.04166667,
        0.04166667,  0.125     ])

p1V

array([ 0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,

        0.        ,  0.05263158,  0.        ,  0.05263158,  0.        ,
        0.        ,  0.        ,  0.05263158,  0.05263158,  0.05263158,
        0.05263158,  0.05263158,  0.        ,  0.10526316,  0.        ,
        0.05263158,  0.05263158,  0.        ,  0.10526316,  0.        ,
        0.15789474,  0.        ,  0.05263158,  0.        ,  0.        ,
        0.        ,  0.        ])



```
